---
title: 'Práctica 2: Analisis y Limpieza de un Conjunto de Datos obtenidos por Web
  Scrapping'
date: "`r Sys.Date()`"
output:
  html_document: default
---

```{r}
library(stringr)
library(skimr)
library(dplyr)
library(ggplot2)
library(fastDummies)
library(randomForest)
library(car)
library(cluster)
library(FNN)
library(dbscan)
```

# Carga de datos

Para el desarrollo de la práctica se va a utilizar el dataset obtenido en la práctica de Web Scraping, el cual cuenta con 541 registros y 22 variables. Se han hecho unas modificaciones respecto al dataset de la primera práctica, puesto que se han añadido 3 columnas más que nos proporcionan más información, como son el título de la propiedad y la url del mismo.

```{r setup}
properties_spain <- read.csv("../properties_Spain.csv")
```

# Limpieza de los datos

## Análisis preliminar de los datos

```{r}
summary(properties_spain)
```

```{r}
nrow(unique(properties_spain))
```
Este dataset contiene columnas numéricas y categóricas, pero, podemos intuir por el resumen anterior que hay variables, como los baños o las habitaciones que deberían ser numéricas y que, por tanto, deberían ser modificadas. Por otro lado, la columna "item" nos muestra que todos los valores son "Na's" por lo que hay que eliminarla. Vemos también que hay registros con valores nulos que no aparecen en el resumen, por lo que habrá que analizar que está pasando para poder cuantificarlo de forma correcta, antes del análisis final. Finalmente podemos ver que no hay registros duplicados, puesto que, si recordamos la práctica 1, lo que hicimos fue traernos todas las propiedades vigentes en venta o alquiler de la página web de Tecnocasa.


Lo primero que hacemos es cambiar el formato a las columnas que sabemos que son numéricas, como son el número de baños, las habitaciones y el consumo de energía. También ponemos en formato fecha la columna de fecha de publicación.

```{r}
properties_spain$Bathrooms <- as.factor(str_extract(properties_spain$Bathrooms, "\\d+"))
properties_spain$Bedrooms <- as.factor(str_extract(properties_spain$Bedrooms, "\\d+"))
properties_spain$Energy_Consumption <- as.numeric(str_extract(properties_spain$Energy_Consumption, "\\d+"))
properties_spain$Publish_date <- as.Date(properties_spain$Publish_date)
```

A continuación, de la columna del título de la página extraemos si que tipo de propiedad es, en el sentido de si es un piso, una casa o una plaza de garaje. 

```{r}
Property <- sapply(properties_spain$title, function(x) strsplit(x, " ")[[1]][1])
properties_spain$Property <- as.data.frame(Property)
```

En este punto eliminamos las columnas que hemos utilizado para obtener la propiedad de cada registro.

```{r}
properties_spain <- subset(properties_spain, select = -c(item, url, title))
```

A continuación, transformamos todos los registros vacíos para que aparezcan como "Na's" debido a que no aparecían de ese modo y nos puede servir para identificar mejor como es el conjunto de datos.

```{r}
properties_spain <- as.data.frame(lapply(properties_spain, function(x) {
  x[x == ""] <- NA
  return(x)
}))
```

Utilizando la fórmula *skim* de la librería *skimr* se va a realizar un análisis exploratorio con el fin de comprobar en que estado está cada variable, cuantos valores perdidos hay, como es la distribución, entre otras cosas.  

```{r}
skim(properties_spain)
```
Podemos ver, por ejemplo, que en las variables categóricas tenemos bastantes valores incompletos, siendo más acusado el ascensor y la calefacción. Por lo que, realmente habrá que pensar si merece la pena mantener esas variables o si las eliminamos. En cuanto al número de habitaciones y baños, vemos que tenemos en torno al 80% de cada variable completado, por lo que quizá podemos utilizar técnicas de imputación de valores para suponer cual es el valor más probable de cada uno, 


## Análisis variables categóricas

### Propiedad

```{r}
frecuencias <- sort(table(properties_spain$Property), decreasing = TRUE)
frecuencias
```


```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Propiedades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,       
        cex.names = 0.8)

```

En vista de los tipos de propiedades que se han extraído, creo que las propiedades "Box/Plaza" y "Local" nos pueden dar problemas en el análisis, por lo que esos registros se van a despreciar y eliminar del dataset. Por otro lado, vemos que sólo hay 1 "Ático", por lo que se va a agrupar dentro de los pisos para el posterior análisis.

```{r}
properties_spain <- properties_spain %>%
                      mutate(Property = ifelse(Property == "Ático", "Piso", Property))
properties_spain <- properties_spain[properties_spain$Property != "Local" & properties_spain$Property != "Box/plaza",]
```

```{r}
frecuencias <- sort(table(properties_spain$Property), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Propiedades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 1,       
        cex.names = 0.8)

```


Además, en vista de estos datos se va a transformar en factor de cara a futuros análisis.

```{r}
properties_spain$Property <- as.factor(properties_spain$Property)
```


### Tipo de Contrato

Del tipo de contrato no hay nada que señalar, puesto que son las casuísticas que esperábamos. Pero si que vemos hay una relación 1:4 en cuanto a los contratos de venta y alquiler en el diagrama siguiente, por lo que igual hay que tener en cuenta esta descompensación de cara al análisis posterior.

```{r}
frecuencias <- sort(table(properties_spain$Contrat), decreasing = TRUE)
frecuencias
```
```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Contratos", 
        xlab = "Tipo de Contrato", 
        ylab = "Frecuencia", 
        col = colores
        )

```
En este punto tenemos 44 viviendas en alquiler y 438 propiedades a la venta. Habría que analizar si realmente merece la pena mantener las propiedades en alquiler o quedarnos solo con las que están a la venta de forma que podamos analizar el precio de venta en el mercado inmobiliario español. Realmente si que tiene sentido, al menos separar ambos tipos de contrato en distintos datasets porque el precio de venta no es lo mismo que el precio de alquilar.

```{r}
properties_spain_sell <- subset(properties_spain[properties_spain$Contrat == "venta",], select = -c(Contrat))
properties_spain_rent <- subset(properties_spain[properties_spain$Contrat == "alquiler",], select = -c(Contrat))
```

Por tanto, a partir de ahora trabajaremos con el dataset "properties_spain_sell".

### Característica de la propiedad


```{r}
frecuencias <- sort(table(properties_spain_sell$Property_Type), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Caracteristicas de Propiedades",
        ylab = "Frecuencia", 
        col = colores
        )

```

Del tipo de propiedad, esto es interesante, porque puede aportarnos información adicional, habrá que ver si realmente es. Se puede ver en el diagrama de barras que mayoritariamente las características de propiedades que hay son: media y popular, lo que indica que son pisos y casas asequibles para la mayoría de las personas. Las casas señoriales y de época suelen ser más caras puesto que precisan de un mayor mantenimiento y cuidados y suelen ser más grandes, lo que implica más dinero y menos posibilidades de venta.


### Provincia

```{r}
frecuencias <- sort(table(properties_spain_sell$City), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

par(mar = c(12, 4, 4, 2))
barplot(frecuencias, 
        main = "Distribución de Ciudades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,
        cex.axis = 0.8
        )
```
En vista de los datos, no tiene mucho sentido mantener la ciudad en el análisis porque el número de inmuebles es muy reducido en cada ciudad y puede repercutir negativamente en el análisis.



### Comunidad autónoma

```{r}
frecuencias <- sort(table(properties_spain_sell$Autonomous_Community), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

par(mar = c(12, 4, 4, 2))
barplot(frecuencias, 
        main = "Distribución de Comunidades Autónomas", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,
        cex.axis = 0.8
        )
```


### Clase Energética


```{r}
frecuencias <- sort(table(properties_spain_sell$Energy_Class), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))


barplot(frecuencias, 
        main = "Distribución de clases energéticas", 
        ylab = "Frecuencia", 
        col = colores,
        cex.axis = 0.8
        )

```

Este punto es más curiosidad que otra cosa, porque de la clase energética nos faltan más de la mitad de los valores, lo que ya limita un análisis más profundo. Sin embargo, entre los datos disponibles, es interesante observar que la mayoría de las viviendas en venta tienen una clasificación energética "e". Esto podría reflejar el estado general del parque inmobiliario en España, donde muchas propiedades en el mercado son construcciones más antiguas o viviendas que no han sido modernizadas para cumplir con los estándares energéticos actuales.

No se va a tener en cuenta para análisis posteriores.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -Energy_Class)
```


### Zona, Calle y País

Estas variables, realmente, para el análisis no aportan mucho, porque estaban pensadas de cara a su visualización, por lo que se van a eliminar del dataset, puesto que no aportan nada en en análisis de datos posterior.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Zone, Street, Country))
```

### Calefación y Ascensor


```{r}
sort(table(properties_spain_sell$Heating), decreasing = TRUE)
```

```{r}
sort(table(properties_spain_sell$Elevator), decreasing = TRUE)
```

A mi forma de ver son variables interesantes de tener en cuenta puesto que se puede analizar si el tipo de calefacción del edificio (sobre todo en comunidades de pisos) y si tiene ascensor pueden incrementar el precio de venta, pero vemos que en cada uno tenemos en torno a un 20% de datos. Posiblemente dependa de la extracción de datos y los valores vacíos sean un "No", pero en vista de que nos falta información, no podemos tener en cuenta estas variables en el análisis posterior.


```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Heating, Elevator))
```



### Planta

```{r}
sort(table(properties_spain_sell$Floor), decreasing = TRUE)
```

Aquí señalar que se podría hacer una limpieza de estos datos para unificarlos porque actualmente no se puede hacer nada con ellos, pero debido a que faltan la mitad de los registros, podemos eliminar esta variable del dataset, de forma que lo simplificariamos un poco más.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Floor))
```

## Análisis variables numéricas

### Referencia

Este columna es un valor unico que se utiliza en la web de Tecnocasa como idnetificador. No nos sirve para el análisis y se puede eliminar. 

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Reference))
```

### Fecha de Publicación

Este columna es un valor que registra cuando se publicó en la web. Realmente está en este dataset para luego ampliar el análisis y considerar si sigue publicado o no, por lo que se va a eliminar del dataset

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Publish_date))
```

### Energy_Consumption

```{r}
summary(properties_spain_sell$Energy_Consumption)
```

Si que me hubiera gustado meter esta variable en el conjunto de entrenamiento, pero faltan la mitad de los datos por lo que no merece la pena, además, posiblemente el consumo energético dependa más de cada habitante de la propiedad que las métricas que de la inmobiliaria, por lo que se va a eliminar del dataset.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Energy_Consumption))
```

### Superficie Construida

```{r}
variable <- properties_spain_sell$Surface

hist(variable, 
     breaks = 50,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Superficie Construida", 
     xlab = "Superficie Construida", 
     ylab = "Densidad")

lines(density(variable, na.rm = TRUE), 
      col = "red",          
      lwd = 2)             

```

Como vemos tenemos una gráfica con una gran cola a la derecha, por lo que se van a ver los outliers y se va a proceder a eliminarlos del dataset.

```{r}
boxplot(properties_spain_sell$Surface, 
        main = "Boxplot de la superficie construida", 
        ylab = "Superficie", 
        col = "lightblue", 
        notch = TRUE)

```

En vista de estos datos, vamos a eliminar los registros que tienen una superficie inferior a 500 m2 y volveremos a analizar todo a ver como queda de cara que luego podamos analizar estudiar los outliers de forma multivariante.

```{r}
properties_spain_sell <- properties_spain_sell[properties_spain_sell$Surface <= 500,]
```

```{r}
variable <- properties_spain_sell$Surface

hist(variable, 
     breaks = 50,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Superficie Construida", 
     xlab = "Superficie Construida", 
     ylab = "Densidad")

lines(density(variable, na.rm = TRUE), 
      col = "red",          
      lwd = 2)             

```

```{r}
boxplot(properties_spain_sell$Surface, 
        main = "Boxplot de la superficie construida", 
        ylab = "Superficie", 
        col = "lightblue", 
        notch = TRUE)

```

Sigue habiendo outliers, pero hemos eliminado gran parte de los mismos, así que por ahora, se va a dejar asi de cara a hacer un análisis de outliers dentro del marco de un dataset multivariante.



### Año de construcción

```{r}
summary(properties_spain_sell$Year_Construction)
```


```{r}
variable <- properties_spain_sell$Year_Construction 


hist(variable, 
     breaks = 100,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Año de Construcción", 
     xlab = "Año de Construcción", 
     ylab = "Densidad")

# Añadir la curva suavizada (density)
lines(density(variable, na.rm = TRUE), 
      col = "red",          # Color de la curva
      lwd = 2)              # Ancho de la línea

```


Respecto a esta variable, vemos que mayoritariamente los pisos se construyeron entre 1960 y los 1980, condicionando con el boom económico español. También se ve reflejada la crisis del 2008 donde se disminuyo dramáticamente la construcción de nuevas propiedades y como a día de hoy todavía no ha vuelto a repuntar. 

```{r}
boxplot(properties_spain_sell$Year_Construction, 
        main = "Boxplot del Año de Construcción", 
        ylab = "Año de Construcción", 
        col = "lightblue", 
        notch = TRUE)
```

Nos encontramos con varios outliers en este dataset respecto al año de construcción, al ser pocos podemos eliminarnos del modelo en el analisis de outliers multivariente posterior y vemos que nos faltan 3 años, los cuales podremos eliminarlos antes del análisis de outliers.


```{r}
properties_spain_sell <- properties_spain_sell[!is.na(properties_spain_sell$Year_Construction),]
```

### Baños


```{r}
summary(properties_spain_sell$Bathrooms)
```
Tenemos 60 valores nulos en esta variable de 370, lo que equivale a un 16% del dataset, por lo que se puede imputar los valores con el fin de completar esta variable. Al estar con una variable tipo factor, podemos usar o la moda (donde imputariamos todos los registros a que tengan 1 baño) o un método predictivo como el k-NN.

Posiblemente, la imputación con la moda es probablemente el enfoque más sencillo y el más fácil de usar en este caso, puesto que la variable Bathrooms tiene pocos factores distintos.

```{r}
mode_bathrooms <- as.numeric(names(sort(table(properties_spain_sell$Bathrooms), decreasing = TRUE)[1]))

properties_spain_sell$Bathrooms[is.na(properties_spain_sell$Bathrooms)] <- mode_bathrooms
```

```{r}
summary(properties_spain_sell$Bathrooms)
```
```{r}
ggplot(properties_spain_sell, aes(x = Bathrooms, fill = factor(Bathrooms))) +
  geom_bar() +
  xlab("Baños") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de baños por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Baños")
```


### Habitaciones


```{r}
summary(properties_spain_sell$Bedrooms)
```
Tenemos 53 valores nulos en esta variable de 370, lo que equivale a un 13% del dataset. 

```{r}
ggplot(properties_spain_sell, aes(x = Bedrooms, fill = factor(Bedrooms))) +
  geom_bar() +
  xlab("Habitaciones") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de Habitaciones por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Habitaciones")
```

Al igual que en el caso anterior, se va a imputar el valor "Na" por la moda del número de habitaciones, el cual es 3, en este caso. 

```{r}
moda <- as.numeric(names(sort(table(properties_spain_sell$Bedrooms), decreasing = TRUE)[1]))

properties_spain_sell$Bedrooms[is.na(properties_spain_sell$Bedrooms)] <- moda

properties_spain_sell$Bedrooms <- droplevels(properties_spain_sell$Bedrooms)
```


```{r}
summary(properties_spain_sell$Bedrooms)
```


```{r}
ggplot(properties_spain_sell, aes(x = Bedrooms, fill = factor(Bedrooms))) +
  geom_bar() +
  xlab("Habitaciones") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de Habitaciones por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Habitaciones")
```



### Precio de Venta

El precio de venta es la variable objetivo.

```{r}
summary(properties_spain_sell$Sale_Price)
```
Vemos, en este punto que el máximo está muy lejos del tercer cuartil, por lo que se va a tener que eliminar, porque debe ser un punto atípico. Veamos antes como son las distribución de esta varaible.


```{r}
variable <- properties_spain_sell$Sale_Price

hist(variable, 
     breaks = 100,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución del Precio de Venta", 
     xlab = "Precio de venta", 
     ylab = "Densidad")

# Añadir la curva suavizada (density)
lines(density(variable, na.rm = TRUE), 
      col = "red",          # Color de la curva
      lwd = 2)              # Ancho de la línea

```

Lo que se puede ver en esta variables es que la mayoría de las viviendas tienen un precio de hasta 40.000 €, lo cual puede ser indicativo del estado, año de construcción u otras características...

```{r}

boxplot(properties_spain_sell$Sale_Price, 
        main = "Boxplot del Precio de Venta", 
        ylab = "Precio de Venta", 
        col = "lightblue", 
        notch = TRUE)

```

Vemos que existen outliers de manera única, muy alejados del resto, por lo que se van a eliminar.

```{r}
Q1 <- quantile(properties_spain_sell$Sale_Price, 0.25)
Q3 <- quantile(properties_spain_sell$Sale_Price, 0.75)
IQR_value <- IQR(properties_spain_sell$Sale_Price)

# Definir los límites inferior y superior
lower_limit <- Q1 - 1.5 * IQR_value
upper_limit <- Q3 + 1.5 * IQR_value

# Eliminar los valores atípicos (outliers)
properties_spain_sell <- properties_spain_sell[properties_spain_sell$Sale_Price >= lower_limit & properties_spain_sell$Sale_Price <= upper_limit,]
```

```{r}
variable <- properties_spain_sell$Sale_Price

hist(variable, 
     breaks = 100,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución del Precio de Venta", 
     xlab = "Precio de venta", 
     ylab = "Densidad")

# Añadir la curva suavizada (density)
lines(density(variable, na.rm = TRUE), 
      col = "red",          # Color de la curva
      lwd = 2)              # Ancho de la línea

```

```{r}

boxplot(properties_spain_sell$Sale_Price, 
        main = "Boxplot del Precio de Venta", 
        ylab = "Precio de Venta", 
        col = "lightblue", 
        notch = TRUE)

```


## Tendencias en función del precio de venta

### Propiedad

```{r}
ggplot(properties_spain_sell, aes(x = Property, y = Sale_Price, fill = Property)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por tipo de Inmueble a la Venta",
    x = "Inmueble",
    y = "Precio de Venta (€)",
    fill = "Inmueble"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 1), 
    plot.title = element_text(hjust = 0.5, size = 16)
  )
```

Es curioso como las casas tienen una mayor dispersión de precios de venta que los pisos, pero luego, tienen una mediana parecida. Esto viene influido, por ejemplo, por si están en un pueblo o una ciudad y su demanda. 

### Característica del inmueble

```{r}
ggplot(properties_spain_sell, aes(x = Property_Type, y = Sale_Price, fill = Property_Type)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por tipo de Inmueble a la Venta",
    x = "Inmueble",
    y = "Precio de Venta (€)",
    fill = "Inmueble"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 16)
  )
```
El gráfico presentado muestra la distribución de los precios de venta en función del tipo de inmueble, diferenciados en cuatro categorías: "De época", "Media", "Popular" y "Señorial". Cada categoría está representada mediante un boxplot, que ilustra la mediana, los rangos intercuartílicos y los valores atípicos para los precios de venta en euros. Se observa que los inmuebles de tipo "Señorial" presentan los precios más altos, con una mayor dispersión, lo que indica una amplia variabilidad en esta categoría. Por otro lado, los inmuebles "Populares" tienen precios más bajos y una dispersión más reducida, lo que refleja mayor homogeneidad en los valores. Este gráfico sugiere que el tipo de inmueble es una variable importante que podría influir significativamente en el análisis y predicción del precio de venta, ya que existen diferencias notables entre las categorías.

### Provincia

```{r}
ggplot(properties_spain_sell, aes(x = City, y = Sale_Price, fill = City)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por Ciudad",
    x = "Ciudad",
    y = "Precio de Venta (€)",
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "none"
  )
```
El gráfico muestra la distribución de los precios de venta de propiedades según la ciudad. Cada boxplot representa una ciudad, permitiendo observar la mediana, la dispersión (rango intercuartílico) y los valores atípicos para los precios de las propiedades en cada localidad. 

Se destacan ciudades como **Madrid** y **Barcelona**, donde los precios son significativamente más altos y presentan mayor dispersión, indicando una variabilidad considerable en el mercado inmobiliario. Por el contrario, ciudades como **Toledo** o **Huelva** muestran precios más bajos y una dispersión menor, lo que indica mayor homogeneidad en los valores. 

Los valores atípicos (representados como puntos rojos) se observan en ciudades como **Madrid**, **Barcelona** y **Bilbao**, reflejando la presencia de propiedades excepcionalmente caras en estas localidades. 

Este gráfico permite visualizar cómo los precios de las propiedades varían ampliamente según la ciudad, lo que sugiere que la ubicación geográfica es un factor determinante en el análisis del mercado inmobiliario.

### Comunidad Atónoma

```{r}
ggplot(properties_spain_sell, aes(x = Autonomous_Community, y = Sale_Price, fill = Autonomous_Community)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por Comunidad Autónoma",
    x = "Ciudad",
    y = "Precio de Venta (€)",
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "none"
  )
```

Se observa que la **Comunidad de Madrid** y el **País Vasco** presentan las distribuciones de precios más altas, con una amplia dispersión, lo que indica una alta variabilidad en los precios de las propiedades. Por el contrario, comunidades como **Extremadura**, **Andalucía**, **Castilla-La Mancha** muestran precios significativamente más bajos y menor dispersión, lo que refleja un mercado inmobiliario más accesible y homogéneo.

Los puntos rojos representan valores atípicos, que corresponden a propiedades con precios excepcionalmente altos en comparación con la mayoría. Este gráfico evidencia que la ubicación geográfica, a nivel de comunidad autónoma, es un factor determinante en los precios del mercado inmobiliario, destacándose las diferencias entre regiones como Madrid y el País Vasco frente a otras comunidades con mercados menos costosos.

### Superficie construida

```{r}
ggplot(properties_spain_sell, aes(x = Surface, y = Sale_Price)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "lm", formula = y ~ x, color = "red", se = FALSE) + 
  labs(
    title = "Precio de Venta en función de la Superficie Construida",
    x = expression("Superficie Construida (m"^2*")"),
    y = "Precio de Venta (€)"
  ) +
  theme_minimal()
```
El gráfico muestra una relación positiva entre el **precio de venta** y la **superficie construida**, donde a mayor superficie, el precio tiende a ser más alto, como lo indica la línea roja de tendencia. Sin embargo, existe una alta dispersión en los precios para superficies similares, lo que sugiere la influencia de otros factores, como la ubicación o las características adicionales del inmueble. También se observan valores extremos para propiedades con grandes superficies, posiblemente reflejando inmuebles de lujo o con características únicas. Este análisis confirma que la superficie es un factor importante, aunque no único, en la determinación del precio de venta.

### Año de construcción

```{r}
ggplot(properties_spain_sell, aes(x = Year_Construction, y = Sale_Price)) +
  geom_point(alpha = 0.5, color = "blue") + 
  labs(
    title = "Precio de Venta en función del Año de Construcción",
    x = "Año de Construcción",
    y = "Precio de Venta (€)"
  ) +
  theme_minimal()
```

El gráfico muestra la relación entre el **precio de venta** y el **año de construcción** de las propiedades. No se observa una tendencia clara que indique una relación directa entre ambas variables, ya que los precios presentan una alta variabilidad en propiedades de todas las épocas. Aunque las construcciones recientes (posteriores a 1960) son más frecuentes y muestran precios mayormente moderados, las propiedades antiguas tienen precios que van desde bajos hasta muy altos, posiblemente debido a su ubicación, características únicas o valor histórico. Esto sugiere que el año de construcción, por sí solo, no es un factor determinante en el precio, aunque podría ser relevante en combinación con otras variables como la ubicación o el tamaño.

### Baños

```{r}
ggplot(properties_spain_sell, aes(x = Bathrooms, y = Sale_Price, fill = Bathrooms)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por cantidad de Baños en el Inmueble",
    x = "Nº de Baños",
    y = "Precio de Venta (€)",
    fill = "Baños"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 1), 
    plot.title = element_text(hjust = 0.5, size = 16)
  )
```

El gráfico muestra la distribución de los precios de venta en función de la cantidad de baños en los inmuebles. Se observa una tendencia creciente en los precios a medida que aumenta el número de baños. Las propiedades con un solo baño presentan precios más bajos y menor dispersión, mientras que las propiedades con dos baños tienen precios más altos y una dispersión intermedia. Las propiedades con tres baños tienen los precios más elevados y la mayor variabilidad, lo que sugiere que los inmuebles más amplios o de mayor categoría tienden a tener más baños. Este patrón indica que el número de baños es un factor relevante en la determinación del precio de venta de las propiedades.

### Habitaciones


```{r}
ggplot(properties_spain_sell, aes(x = Bedrooms, y = Sale_Price, fill = Bedrooms)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Distribución de los Precios de Venta por números de Habitaciones en el Inmueble",
    x = "Nº de Habitaciones",
    y = "Precio de Venta (€)",
    fill = "Habitaciones"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 1), 
    plot.title = element_text(hjust = 0.5, size = 16)
  )
```

El gráfico muestra la distribución de los precios de venta según el número de habitaciones en los inmuebles. En general, se observa que el precio tiende a aumentar con un mayor número de habitaciones, especialmente para propiedades con 7 y 9 habitaciones, que presentan precios significativamente más altos. Sin embargo, las propiedades con 6 habitaciones muestran precios notablemente más bajos, lo que podría indicar un comportamiento atípico o diferencias en otras características, como ubicación o tamaño. También se nota una gran dispersión en los precios para inmuebles con 3, 4 y 5 habitaciones, lo que sugiere que otros factores además del número de habitaciones influyen en el precio. Este patrón confirma que el número de habitaciones es un factor relevante.


## Analisis del dataset limpio

Para simplificar el análisis se va a eliminar la variable ciudad puesto que se puede englobar dentro de la variable comunidad autónoma. 

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(City))
```


```{r}
skim(properties_spain_sell)
```
Vemos que no hay inmuebles con 4 baños, por lo que vamos a eliminarlo del factor.

```{r}
properties_spain_sell$Bathrooms <- droplevels(properties_spain_sell$Bathrooms)
```


### Detección de Outliers: PCA y distancia de Mahalanobis

Para analizar los outliers de este dataset de forma conjunta se va a utilizar el análisis de componentes principales (PCA, por sus siglas en inglés). Vamos a convertir cada variable factor en una columna binaria para evitar suposiciones de orden.


```{r}
properties_spain_encoded <- dummy_cols(properties_spain_sell, 
                                       select_columns = c("Bathrooms", "Bedrooms", "Property", "Property_Type", "Autonomous_Community"),
                                       remove_selected_columns = TRUE)

# Seleccionar las variables numéricas
numeric_vars <- properties_spain_encoded[, c("Year_Construction", "Sale_Price", "Surface")]

# Normalizar las variables
numeric_vars_scaled <- scale(numeric_vars)

# Reemplazar en el dataset
properties_spain_encoded[, c("Year_Construction", "Sale_Price", "Surface")] <- numeric_vars_scaled

```


```{r}
# Aplicar PCA
pca_result <- prcomp(properties_spain_encoded, scale. = TRUE)

# Resumen de la varianza explicada
summary(pca_result)
``` 


```{r}
# Obtener la desviación estándar de cada componente
sdev <- pca_result$sdev

# Calcular la varianza explicada (al cuadrar la desviación estándar)
var_explained <- sdev^2

# Calcular la proporción de varianza explicada
prop_var_explained <- var_explained / sum(var_explained)

# Calcular la varianza acumulada
cumulative_var_explained <- cumsum(prop_var_explained)

# Graficar la varianza acumulada
plot(cumulative_var_explained, type="b", main="Cumulative Proportion of Variance Explained",
     xlab="Principal Components", ylab="Cumulative Proportion of Variance", col="blue", pch=19)

```


```{r}
# Obtener la desviación estándar de cada componente
sdev <- pca_result$sdev

# Calcular la varianza explicada (al cuadrar la desviación estándar)
var_explained <- sdev^2

# Calcular la proporción de varianza explicada
prop_var_explained <- var_explained / sum(var_explained)

plot(prop_var_explained, type="b", pch=19, col="blue", xlab="Número de componentes principales", 
     ylab="Proporción de varianza explicada", main="Gráfico de la proporción de varianza explicada")

```

En vista de este análisis 20 PCA explicarían el 90% de la varianza de los datos y podríamos usarlo para eliminar los outliers del modelo.


```{r}
pca_data <- data.frame(pca_result$x[, 1:20])
```


```{r}
# Calcular la matriz de covarianza y la distancia de Mahalanobis
cov_matrix <- cov(pca_data) 

# Calcular la media de las puntuaciones de PCA
mean_data <- colMeans(pca_data)

# Calcular la distancia de Mahalanobis para cada observación
mahal_dist <- mahalanobis(pca_data, center = mean_data, cov = cov_matrix)

# Definir el umbral usando la distribución chi-cuadrada
threshold <- qchisq(0.95, df = 20) 

# Identificar los outliers
outliers <- mahal_dist > threshold

# Ver los índices de los outliers
which(outliers)
```

Dibujamos algunos de los PCA para ver como se reparten los outliers, previo borrado del dataset.

```{r}
pca_data$outlier <- outliers
```

```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 1 (PC1)",
    y = "Componente Principal 2 (PC2)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```
```{r}
ggplot(pca_data, aes(x = PC1, y = PC3, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 1 (PC1)",
    y = "Componente Principal 3 (PC3)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```
```{r}
ggplot(pca_data, aes(x = PC2, y = PC3, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 2 (PC2)",
    y = "Componente Principal 3 (PC3)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```

Si que vemos en las gráficas que hay una gran dispersión en los datos atípicos que pueden distorsionarnos los resultados, por lo que, finalmente eliminamos los outliers del dataset para poder trabajar con en los posteriores análisis.



```{r}
cleaned_properties <- properties_spain_sell[!outliers, ]
```

Vemos, tras el análisis, que tenemos varios puntos que son outliers en nuestro dataset y que, por tanto, debemos eliminar del modelo antes de proceder a realizar el análisis de los datos. Además lo borramos del dataset que ya tenemos preparado con todas las variables numéricas, lo cual nos va a servir para aplicar los modelos de análisis posteriores.

```{r}
skim(cleaned_properties)
```

Vemos que no hay inmuebles con 1 habitación, por lo que vamos a eliminarlo del factor.

```{r}
cleaned_properties$Bedrooms <- droplevels(cleaned_properties$Bedrooms)
```


```{r}
write.csv(cleaned_properties, file = "../cleaned_properties_Spain.csv", row.names = FALSE)
```


# Análisis de los datos

El objetivo de este análisis es predecir una variable numérica continua ("Sale_Price"), por lo que los métodos adecuados podrían ser:

- Métodos Supervisados 
  1. Regresión Lineal Múltiple
  2. Árboles de Decisión
  3. Random Forest
  4. Gradient Boosting
  5. Red Neuronal

## Separación de la variable objetivo y las variables predictoras

```{r}
obs_X <- subset(cleaned_properties, select = c(-Sale_Price))
obj_Y <- subset(cleaned_properties, select = c(Sale_Price))
```

## Reescalado de las variables predictoras

```{r}
obs_X_encoded <- dummy_cols(obs_X, 
                            select_columns = c("Bathrooms", "Bedrooms", "Property", "Property_Type", "Autonomous_Community"),
                            remove_first_dummy = TRUE,  
                            remove_selected_columns = TRUE)

# Seleccionar las variables numéricas
numeric_vars <- obs_X_encoded[, c("Year_Construction", "Surface")]

# Normalizar las variables
numeric_vars_scaled <- scale(numeric_vars)

# Reemplazar en el dataset
obs_X_encoded[, c("Year_Construction", "Surface")] <- numeric_vars_scaled

colnames(obs_X_encoded) <- make.names(colnames(obs_X_encoded))

```


## Colinealidad de las variables
  

Lo primero que vamos a analizar es si con las variables que tenemos, presentamos colinealidad con el fin de eliminarla antes de realizar los métodos supervisados y no supervisados.


```{r}
cor(obs_X_encoded)
```
```{r}
data_encoded <- obs_X_encoded
data_encoded$Sale_Price <- obj_Y$Sale_Price
colnames(data_encoded) <- make.names(colnames(data_encoded))
vif(lm(Sale_Price ~ ., data = data_encoded))
```
Si analizamos el VIF y la matriz de colinealidad, vemos que se presenta colinealidad entre los datos elegidos, sobre todo en las vairables relativas a las habitaciones y al tipo de propiedad, por lo que se van a eliminar del modelo.

```{r}
data_encoded <- data_encoded[, !colnames(data_encoded) %in% c("Bedrooms_2", "Bedrooms_3", "Bedrooms_4", "Property_Type_Media", "Property_Type_Popular")]

obs_X_encoded <- obs_X_encoded[, !colnames(obs_X_encoded) %in% c("Bedrooms_2", "Bedrooms_3", "Bedrooms_4", "Property_Type_Media", "Property_Type_Popular")]
```

```{r}
model <- lm(Sale_Price ~ ., data = data_encoded)
vif_values <- vif(model)
print(vif_values)
```
Tras realizar un análisis de multicolinealidad utilizando el Variance Inflation Factor (VIF), se han eliminado las variables que presentaban valores extremadamente altos, lo que indicaba redundancia entre ellas y posibles problemas para los modelos predictivos. Después de este ajuste, todas las variables restantes muestran valores de VIF inferiores a 5, lo que sugiere una multicolinealidad baja o aceptable. Esto garantiza que las variables seleccionadas son independientes entre sí y que no hay relaciones lineales fuertes que puedan distorsionar los resultados del modelo. Con este dataset depurado, ahora es posible proceder con los métodos supervisados y no supervisados para analizar y predecir el precio de venta. 


## Métodos Supervisados

Respecto a los métodos supervisados, se va a aplicar en primera instancia una Regresión Lineal, por tantear el comportamiento de los datos para, después aplicar uno de los otros métodos.

### Regresión Lineal

```{r}
model <- lm(Sale_Price ~ ., data = data_encoded)
summary(model)
```
Aunque no tenemos mucha información de este modelo y, realmente no nos sirve, si que podemos intuir que el numero de baños, si es casa o piso y la superficie construida influyen significativamente en el precio de la vivienda.



```{r}
predicted_prices <- predict(model, newdata = data_encoded)
# Calcular el RMSE
actual_prices <- cleaned_properties$Sale_Price
rmse_lm <- sqrt(mean((actual_prices - predicted_prices)^2))
print(paste("RMSE para la regresión lineal:", rmse_lm))
```
```{r}
ggplot(data = data_encoded, aes(x = actual_prices, y = predicted_prices)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  xlab("Precios reales") +
  ylab("Precios predichos") +
  ggtitle("Comparación de precios reales vs. predichos") +
  theme_minimal()
```

### Random Forest

```{r}
# Función personalizada para explorar ntree y mtry
results <- data.frame()

for (ntree in c(500, 1000, 1500, 2000, 2500)) {
  for (mtry in c(5, 10, 15)) {
    rf_model <- randomForest(Sale_Price ~ ., data = data_encoded, ntree = ntree, mtry = mtry)
    rmse <- sqrt(mean((predict(rf_model, data_encoded) - data_encoded$Sale_Price)^2))
    r_squared <- 1 - sum((predict(rf_model, data_encoded) - data_encoded$Sale_Price)^2) / 
                       sum((mean(data_encoded$Sale_Price) - data_encoded$Sale_Price)^2)
    results <- rbind(results, data.frame(ntree, mtry, RMSE = rmse, R2 = r_squared))
  }
}

# Ver los resultados
print(results)

# Elegir los mejores parámetros
best_params <- results[which.min(results$RMSE), ]
print(best_params)

```
```{r}
set.seed(42)

train_index <- sample(1:nrow(obs_X_encoded), 0.8 * nrow(obs_X_encoded))
train_data <- obs_X_encoded[train_index, ]
test_data <- obs_X_encoded[-train_index, ]

# Variables objetivo para entrenamiento y prueba
train_target <- obj_Y$Sale_Price[train_index]
test_target <- obj_Y$Sale_Price[-train_index]
```

```{r}
# Entrenar el modelo
rf_model <- randomForest(Sale_Price ~ ., data = data_encoded, ntree = 2000, mtry = 15, importance = TRUE)

# Predecir en los datos de prueba
predictions <- predict(rf_model, newdata = test_data)

# Calcular métricas de rendimiento
rmse <- sqrt(mean((predictions - test_target)^2))
r_squared <- 1 - sum((predictions - test_target)^2) / sum((mean(test_target) - test_target)^2)

cat("RMSE:", rmse, "\n")
cat("R²:", r_squared, "\n")

# Evaluar importancia de variables
importance(rf_model)


```
```{r}
predicted_prices <- predict(rf_model, newdata = test_data)

ggplot(data = data.frame(Actual = test_target, Predicted = predicted_prices), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Valores Reales vs Predichos",
       x = "Valor Real",
       y = "Valor Predicho") +
  theme_minimal()
```


El análisis de importancia de las variables en el modelo Random Forest optimizado revela que la **superficie** de la propiedad es, con un 85.90% de incremento en el error (IncMSE) al eliminarla, la variable más influyente en la predicción del precio de venta. Le sigue la pertenencia a la comunidad autónoma de **Madrid** (55.38%), destacando nuevamente el impacto significativo del mercado inmobiliario en esta región. El número de baños también tiene un papel relevante, especialmente para propiedades con tres baños (40.65%) y dos baños (35.20%), lo que confirma que esta característica es clave en la determinación del precio. Además, las propiedades clasificadas como **Piso** (33.19%) y las situadas en **Extremadura** (34.40%) también presentan una influencia considerable, reflejando diferencias regionales y estructurales del mercado. Por otro lado, variables como **Bedrooms_5** (3.37%) y **Property_Type_Señorial** (-0.68%) tienen una influencia baja o nula, lo que sugiere que su relevancia en el modelo es mínima. Este análisis subraya la importancia de variables estructurales y regionales en la predicción del precio de las propiedades.


## Métodos no supervisados

Para los métodos no supervisados: 

Razón para excluir Sale_Price

K-Means crea clústeres basados en la distancia:

Si incluyes Sale_Price, el algoritmo tenderá a agrupar propiedades según esta variable en lugar de considerar un balance de todas las características. Esto podría resultar en clústeres sesgados o poco interpretables.

La variable Sale_Price ya es una función de otras variables:

En tu análisis supervisado, Sale_Price es el resultado de una combinación de las otras variables. Al incluirla en K-Means, estás introduciendo redundancia y potencialmente distorsionando los clústeres.


### K-Means

```{r}
# Calcular la suma de las distancias intra-clúster (inertia) para diferentes k
wss <- sapply(1:15, function(k) {
  kmeans(obs_X_encoded, centers = k, nstart = 25)$tot.withinss
})

# Graficar el método del codo
plot(1:15, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de Clústeres (k)", ylab = "Suma de Cuadrados Intra-Clúster (WSS)")
```

```{r}
predictors <- obs_X_encoded

# Probar diferentes valores de k y calcular la silueta promedio
sil_width <- sapply(2:10, function(k) {
  pam(predictors, k = k)$silinfo$avg.width
})

# Graficar el coeficiente de silueta
plot(2:10, sil_width, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de Clústeres (k)", ylab = "Coeficiente de Silueta")
```
Teniendo en cuenta ambas imágenes, parece que el número de clusteres más adecuado está entre 4 y 6. Estos valores deberían proporcionar un buen balance entre minimizar el WSS y mantener la simplicidad en el modelo.

```{r}
k = 4

kmeans_model <- kmeans(predictors, centers = k, nstart = 25)

# Agregar los clústeres al dataset original
predictors$Cluster <- kmeans_model$cluster
```

```{r}
table(kmeans_model$cluster)
```
```{r}
aggregate(predictors[, -ncol(predictors)], by = list(Cluster = predictors$Cluster), FUN = mean)
```

```{r}
# Visualizar los clústeres en 2D
ggplot(predictors, aes(x = Surface, y = Year_Construction, color = factor(Cluster))) +
  geom_point() +
  labs(color = "Cluster") +
  theme_minimal()
```

```{r}
aggregate(obj_Y$Sale_Price, by = list(Cluster = predictors$Cluster), FUN = mean)
```

El análisis de clustering k-means resultó en una segmentación efectiva de los datos en 4 clústeres, con diferencias claras en el precio promedio de venta de las propiedades. El Clúster 1 muestra un precio promedio de 260,350.00, indicando que agrupa propiedades de mayor valor, probablemente asociadas con características como mayor superficie o mejor ubicación. Por otro lado, el Clúster 3 tiene el precio promedio más bajo, 147,703.40, lo que podría estar relacionado con propiedades más pequeñas o ubicadas en zonas menos costosas. Los Clústeres 2 y 4 presentan precios promedio de 154,551.80 y 183,557.30, respectivamente, reflejando segmentos intermedios en términos de precio. Esta segmentación permite identificar patrones en el mercado inmobiliario y comprender cómo variables como superficie y año de construcción influyen en el valor de las propiedades.

### DBSCAN

```{r}
predictors <- obs_X_encoded
```


```{r}
dbscan_model <- dbscan(predictors, eps = 1, minPts = 5)

print(dbscan_model)

predictors$Cluster <- dbscan_model$cluster

```


```{r}
# Valores de `eps` y `minPts` a probar
eps_values <- seq(0.02, 2, by = 0.02)  # Cambia según el rango esperado de `eps`
minPts_values <- c(5, 10, 15, 20)       # Cambia según el dataset

# Dataframe para almacenar resultados
results <- data.frame(
  eps = numeric(0),
  minPts = numeric(0),
  clusters = integer(0),
  noise_points = integer(0)
)

# Iterar sobre los valores
for (eps in eps_values) {
  for (minPts in minPts_values) {
    predictors <- obs_X_encoded
    # Aplicar DBSCAN
    dbscan_model <- dbscan(predictors, eps = eps, minPts = minPts)
    
    # Contar número de clústeres (excluyendo el ruido, que es cluster 0)
    num_clusters <- length(unique(dbscan_model$cluster[dbscan_model$cluster > 0]))
    
    # Contar puntos de ruido
    noise_points <- sum(dbscan_model$cluster == 0)
    
    # Guardar resultados
    results <- rbind(
      results,
      data.frame(
        eps = eps,
        minPts = minPts,
        clusters = num_clusters,
        noise_points = noise_points
      )
    )
  }
}
```


```{r}
ggplot(results, aes(x = eps, y = clusters, color = factor(minPts))) +
  geom_line() +
  geom_point() +
  labs(
    title = "Número de clústeres vs. eps y minPts",
    x = "eps",
    y = "Número de clústeres",
    color = "minPts"
  ) +
  theme_minimal()
```

```{r}
ggplot(results, aes(x = eps, y = noise_points, color = factor(minPts))) +
  geom_line() +
  geom_point() +
  labs(
    title = "Número de puntos de ruido vs. eps y minPts",
    x = "eps",
    y = "Número de puntos de ruido",
    color = "minPts"
  ) +
  theme_minimal()

```

```{r}
results_filtered <- results[results$clusters > 2,]
# Ver los resultados
min <- min(results_filtered$noise_points)
optimal_config <- results_filtered[results_filtered$noise_points == min,]
print(optimal_config)
```


```{r}
optimal_eps <- optimal_config$eps[1]
optimal_minPts <- optimal_config$minPts[1]

dbscan_model <- dbscan(predictors, eps = optimal_eps, minPts = optimal_minPts)
print(dbscan_model)
```

```{r}
predictors$Cluster <- dbscan_model$cluster

ggplot(predictors, aes(x = Surface, y = Year_Construction, color = factor(Cluster))) +
  geom_point() +
  labs(color = "Cluster") +
  theme_minimal()
```

```{r}
aggregate(obj_Y$Sale_Price, by = list(Cluster = predictors$Cluster), FUN = mean)
```
El modelo DBSCAN aplicado a los datos ha identificado **8 clústeres** distintos y **106 puntos de ruido**. Los clústeres se diferencian en función de las características estructurales y regionales de las propiedades, con precios de venta medios que varían considerablemente. Por ejemplo, el clúster 1 contiene la mayor parte de los datos (168 observaciones) con un precio promedio de **146,404.74**, mientras que otros clústeres más pequeños, como el 8 (6 observaciones), presentan un precio promedio significativamente menor (**53,216.67**). Por otro lado, los puntos considerados como ruido tienen un precio promedio de **215,598.58**, lo que indica que podrían corresponder a casos atípicos o propiedades que no encajan en ninguna agrupación. Aunque el modelo ha capturado patrones claros en los datos, el ruido identificado puede ser reducido ajustando los parámetros o preprocesando los datos, si se considera necesario. En general, DBSCAN ha demostrado ser útil para identificar agrupaciones densas y diferenciar propiedades según sus características clave.


## Prueba por contraste de hipótesis

Para esta prueba, lo que se va a contrastar es si realmente hay diferencias entre el *valor promedio* del precio por metro cuadrado entre casas y pisos. Para ello, a partir del data frame con los datos limpios "cleaned_properties" se va a transformar para obtener una columna que sea "propiedad" y otra que sea "precio/m2" 

```{r}
cleaned_properties$Sale_m2 = cleaned_properties$Sale_Price / cleaned_properties$Surface 
```


```{r}
cleaned_properties %>%
  group_by(Property) %>%
  summarize(
    Mean_Sale_m2 = mean(Sale_m2, na.rm = TRUE),
    SD_Sale_m2 = sd(Sale_m2, na.rm = TRUE),
    Median_Sale_m2 = median(Sale_m2, na.rm = TRUE),
    Robust_SD_Sale_m2 = mad(Sale_m2, na.rm = TRUE) # Desviación estándar robusta
  )
```

Las hipótesis deberían ser las que siguen:

$$H_0: \mu_1 = \mu_2$$ 

$$H_1: \mu_1 \neq \mu_2$$

Se va a estudiar el **valor promedio de precio por metro cuadrado** y si hay diferencias en función de distintas variables. 

En caso de que la muestra siguiera una distribución normal, el test de contraste de hipótesis que se va a hacer es un test de contraste bilateral de dos muestras independientes sobre la media de la variable de interés (precio por metro cuadrado) con varianzas desconocidas. Como se desconocen las varianzas, habrá que hacer un test de homoscedasticidad para comprobar si podemos considerar varianzas iguales o diferentes. Pero esto se puede implementar dentro de la función de contraste. Sin embargo, si la muestra no siguiera una distribución normal, el test que se va a realizar es la prueba no paramétrica de Mann-Whitney-Wilcoxon.

```{r}
limitValue = 1.5
alpha = 0.05
  
array1 <- cleaned_properties$Sale_m2[cleaned_properties$Property == "Piso"]
array2 <- cleaned_properties$Sale_m2[cleaned_properties$Property == "Casa"]

array1 <- na.omit(array1)
array2 <- na.omit(array2)

# Comprobar la normalidad de las muestras
shapiro_p1 <- shapiro.test(array1)$p.value
shapiro_p2 <- shapiro.test(array2)$p.value

# Interpretar resultados de normalidad
normality_message <- if (shapiro_p1 > 0.05 & shapiro_p2 > 0.05) {
  "Ambas muestras siguen una distribución normal."
} else {
  "Al menos una de las muestras no sigue una distribución normal."
}

print(c(shapiro_p1, shapiro_p2, normality_message))
```
```{r}
if (shapiro_p1 > 0.05 | shapiro_p2 > 0.05) {
  
  # Aqui comprobamos la homoscedasticidad 
  # para saber si hay que operar con varianzas iguales o no.
    mean1 <- mean(array1); n1 <- length(array1); sd1 <- sd(array1) 
    mean2 <- mean(array2); n2 <- length(array2); sd2 <- sd(array2)
      
    fobs <- sd1^2/sd2^2
      
    # Valores mínimo y máximo y p-valor para la distribución t de Student
      
    fcritL <- qf(alpha/2, df1 = n1-1, df2 = n2-2)
    fcritU <- qf(1-alpha/2, df1 = n1-1, df2 = n2-2)
      
    pvalue_sd <- min(pf(fobs, df1 = n1-1, df2 = n2-2, lower.tail = FALSE), 
                      pf(fobs, df1 = n1-1, df2 = n2-2))*2
      
    
    # Después de calcular el pvalue_sd hay 2 opciones. 
      # 1º p < alpha: No hay varianzas iguales. 
      # 2º p > alpha: Hay varianzas iguales.
    # En función se hace un cálculo u otro del valor t observado
     
    if (pvalue_sd > alpha){
      
      S_value = sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
      tobs = (mean1 - mean2)/(S_value*sqrt((1/n1)+(1/n2))) 
      
    } else{
      
      tobs = (mean1 - mean2)/(sqrt((sd1^2/n1)+(sd2^2/n2))) 
      
    }
    
    # Valores mínimo y máximo y p-valor para la distribución t de Student
    
    tcritL <- qt(alpha/2, n1+n2-2)
    tcritU <- qt(1-alpha/2, n1+n2-2)
    
    pvalue_mean <- pt(abs(tobs), df=n1+n2-2, lower.tail = FALSE)*2
    
    # Una vez obtenido el p-valor de la media, podemos obtener un pequeño mensaje
    # que nos permita saber si se cumple la hipótesis nula o no.
    
    if (pvalue_mean > alpha){
      
      message = "No hay diferencias significativas"
      
    } else{
      
      message = "Hay diferencias significativas"
      
    }
    
    # Se nos devuelve el valor observado, el valor cítico, el p-valor y el mensaje
    
    print(c(round(c(tobs, tcritU, pvalue_mean),3), message))
    
} else {
    wilcox.test(array1, array2, alternative = "two.sided")
}

```
Los resultados de la prueba de Wilcoxon Rank Sum (prueba de Mann-Whitney-Wilcoxon) muestran que dado que el p-value es mucho menor que el nivel de significancia común *alpha = 0.05*, se **rechaza la hipótesis nula** $H_0$. Esto significa que hay diferencias estadísticamente significativas en la mediana del precio por metro cuadrado entre casas y pisos.

```{r}

ggplot(cleaned_properties, aes(x = Property, y = Sale_m2, fill = Property)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.7) +
  labs(
    title = "Distribución del Precio por Metro Cuadrado",
    x = "Tipo de Propiedad",
    y = "Precio por m²"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    legend.position = "none"
  ) +
  scale_fill_manual(values = c("Piso" = "#56B4E9", "Casa" = "#E69F00"))
```

