---
title: "Analisis_Limpieza"
author: "Marina Fernández Delgado"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(stringr)
library(skimr)
library(dplyr)
library(ggplot2)
library(fastDummies)
library(randomForest)
library(car)
library(cluster)
library(FNN)
library(dbscan)
```

# Carga de datos

Para el desarrollo de la práctica se va a utilizar el dataset obtenido en la práctica de Web Scraping, el cual cuenta con 541 registros y 22 variables. Se han hecho unas modificaciones respecto al dataset de la primera práctica, puesto que se han añadido 3 columnas más que nos proporcionan másimformación, como son el título de la propiedad y la url del mismo.

```{r setup}
properties_spain <- read.csv("properties_Spain.csv")
```

# Limpieza de los datos

## Análisis preliminar de los datos

```{r}
summary(properties_spain)
```

```{r}
nrow(unique(properties_spain))
```
Este dataset contiene columnas numéricas y categóricas, pero, podemos intuir por el resumen anterior que hay variables, como los baños o las habitaciones que deberían ser numéricas y que, por tanto, deberían ser modificadas. Por otro lado, la columna "item" nos muestra que todos los valores son "Na's" por lo que hay que eliminarla. Vemos también que hay registros con valores nulos que no aparecen en el resumen, por lo que habrá que analizar que está pasando para poder cuantificarlo de forma correcta, antes del análisis final. Finalmente podemos ver que no hay registros duplicados, puesto que, si recordamos la práctica 1, lo que hicimos fue traernos todas las propiedades vigentes en venta o alquiler de la página web de Tecnocasa.


Lo primero que hacemos es cambiar el formato a las columnas que sabemos que son numéricas, como son el número de baños, las habitaciones y el consumo de energía. También ponemos en formato fecha la columna de fecha de publicación.

```{r}
properties_spain$Bathrooms <- as.factor(str_extract(properties_spain$Bathrooms, "\\d+"))
properties_spain$Bedrooms <- as.factor(str_extract(properties_spain$Bedrooms, "\\d+"))
properties_spain$Energy_Consumption <- as.numeric(str_extract(properties_spain$Energy_Consumption, "\\d+"))
properties_spain$Publish_date <- as.Date(properties_spain$Publish_date)
```

A continuación, de la columna del título de la página extraemos si que tipo de propiedad es, en el sentido de si es un piso, una casa o una plaza de garaje. 

```{r}
Property <- sapply(properties_spain$title, function(x) strsplit(x, " ")[[1]][1])
properties_spain$Property <- as.data.frame(Property)
```

En este punto eliminamos las columnas que hemos utilizado para obtener la propiedad de cada registro.

```{r}
properties_spain <- subset(properties_spain, select = -c(item, url, title))
```

A continuación, transformamos todos los registros vacíos para que aparezcan como "Na's" debido a que no aparecían de ese modo y nos puede servir para identificar mejor como es el conjunto de datos.

```{r}
properties_spain <- as.data.frame(lapply(properties_spain, function(x) {
  x[x == ""] <- NA
  return(x)
}))
```

Utilizando la fórmula *skim* de la librería *skimr* se va a realizar un análisis exploratorio con el fin de comprobar en que estado está cada variable, cuantos valores perdidos hay, como es la distribución, entre otras cosas.  

```{r}
skim(properties_spain)
```
Podemos ver, por ejemplo, que en las variables categóricas tenemos bastantes valores incompletos, siendo más acusado el ascensor y la calefacción. Por lo que, realmente habrá que pensar si merece la pena mantener esas variables o si las eliminamos. En cuanto al número de habitaciones y baños, vemos que tenemos en torno al 80% de cada variable completado, por lo que quizá podemos utilizar técnicas de imputación de valores para suponer cual es el valor más probable de cada uno, 


## Análisis variables categóricas

### Propiedad

```{r}
frecuencias <- sort(table(properties_spain$Property), decreasing = TRUE)
frecuencias
```


```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Propiedades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,       
        cex.names = 0.8)

```

En vista de los tipos de propiedades que se han extraído, creo que las propiedades "Box/Plaza" y "Local" nos pueden dar problemas en el análisis, por lo que esos registros se van a despreciar y eliminar del dataset. Por otro lado, vemos que sólo hay 1 "Ático", por lo que se va a agrupar dentro de los pisos para el posterior análisis.

```{r}
properties_spain <- properties_spain %>%
                      mutate(Property = ifelse(Property == "Ático", "Piso", Property))
properties_spain <- properties_spain[properties_spain$Property != "Local" & properties_spain$Property != "Box/plaza",]
```

```{r}
frecuencias <- sort(table(properties_spain$Property), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Propiedades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,       
        cex.names = 0.8)

```


Además, en vista de estos datos se va a transformar en factor de cara a futuros análisis.

```{r}
properties_spain$Property <- as.factor(properties_spain$Property)
```


### Tipo de Contrato

Del tipo de contrato no hay nada que señalar, puesto que son las casuísticas que esperábamos. Pero si que vemos hay una relación 1:4 en cuanto a los contratos de venta y alquiler en el diagrama siguiente, por lo que igual hay que tener en cuenta esta descompensación de cara al análisis posterior.

```{r}
frecuencias <- sort(table(properties_spain$Contrat), decreasing = TRUE)
frecuencias
```
```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Contratos", 
        xlab = "Tipo de Contrato", 
        ylab = "Frecuencia", 
        col = colores
        )

```
En este punto tenemos 44 viviendas en alquiler y 438 propiedades a la venta. Habría que analizar si realmente merece la pena mantener las propiedades en alquiler o quedarnos solo con las que están a la venta de forma que podamos analizar el precio de venta en el mercado inmobiliario español. Realmente si que tiene sentido, al menos separar ambos tipos de contrato en distintos datasets porque el precio de venta no es lo mismo que el precio de alquilar.

```{r}
properties_spain_sell <- subset(properties_spain[properties_spain$Contrat == "venta",], select = -c(Contrat))
properties_spain_rent <- subset(properties_spain[properties_spain$Contrat == "alquiler",], select = -c(Contrat))
```

Por tanto, a partir de ahora trabajaremos con el dataset "properties_spain_sell".

### Característica de la propiedad


```{r}
frecuencias <- sort(table(properties_spain_sell$Property_Type), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

barplot(frecuencias, 
        main = "Distribución de Caracteristicas de Propiedades", 
        xlab = "Caracteristicas de Propiedades", 
        ylab = "Frecuencia", 
        col = colores
        )

```

Del tipo de propiedad, esto es interesante, porque puede aportarnos información adicional, habrá que ver si realmente es. Se puede ver en el diagrama de barras que mayoritariamente las características de propiedades que hay son: media y popular, lo que indica que son pisos y casas asequibles para la mayoría de las personas. Las casas señoriales y de época suelen ser más caras puesto que precisan de un mayor mantenimiento y cuidados y suelen ser más grandes, lo que implica más dinero y menos posibilidades de venta.

Sin embargo, puede ser una variable que distorsione el modelo por lo que no se va a tener en cuenta. 

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c((Property_Type)))
```

### Ciudad

```{r}
frecuencias <- sort(table(properties_spain_sell$City), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

par(mar = c(12, 4, 4, 2))
barplot(frecuencias, 
        main = "Distribución de Ciudades", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,
        cex.axis = 0.8
        )
```
En vista de los datos, no tiene mucho sentido mantener la ciudad en el análisis porque el número de inmuebles es muy reducido en cada ciudad y puede repercutir negativamente en el análisis.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(City))
```

### Comunidad autónoma

```{r}
frecuencias <- sort(table(properties_spain_sell$Autonomous_Community), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))

par(mar = c(12, 4, 4, 2))
barplot(frecuencias, 
        main = "Distribución de Comunidades Autónomas", 
        ylab = "Frecuencia", 
        col = colores,
        las = 2,
        cex.axis = 0.8
        )
```
Sin embargo, para evitar un dataset demasiado grande, se va a eliminar del modelo.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Autonomous_Community))
```

### Clase Energética


```{r}
frecuencias <- sort(table(properties_spain_sell$Energy_Class), decreasing = TRUE)
frecuencias
```

```{r}
colores <- rainbow(length(frecuencias))


barplot(frecuencias, 
        main = "Distribución de clases energéticas", 
        ylab = "Frecuencia", 
        col = colores,
        cex.axis = 0.8
        )

```

Este punto es más curiosidad que otra cosa, porque de la clase energética nos faltan más de la mitad de los valores, lo que ya limita un análisis más profundo. Sin embargo, entre los datos disponibles, es interesante observar que la mayoría de las viviendas en venta tienen una clasificación energética "e". Esto podría reflejar el estado general del parque inmobiliario en España, donde muchas propiedades en el mercado son construcciones más antiguas o viviendas que no han sido modernizadas para cumplir con los estándares energéticos actuales.

No se va a tener en cuenta para análisis posteriores.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -Energy_Class)
```


### Zona, Calle y País

Estas variables, realmente, para el análisis no aportan mucho, porque estaban pensadas de cara a su visualización, por lo que se van a eliminar del dataset, puesto que no aportan nada en en análisis de datos posterior.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Zone, Street, Country))
```

### Calefación y Ascensor


```{r}
sort(table(properties_spain_sell$Heating), decreasing = TRUE)
```

```{r}
sort(table(properties_spain_sell$Elevator), decreasing = TRUE)
```

A mi forma de ver son variables interesantes de tener en cuenta puesto que se puede analizar si el tipo de calefacción del edificio (sobre todo en comunidades de pisos) y si tiene ascensor pueden incrementar el precio de venta, pero vemos que en cada uno tenemos en torno a un 20% de datos. Posiblemente dependa de la extracción de datos y los valores vacíos sean un "No", pero en vista de que nos falta información, no podemos tener en cuenta estas variables en el análisis posterior.


```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Heating, Elevator))
```



### Planta

```{r}
sort(table(properties_spain_sell$Floor), decreasing = TRUE)
```

Aquí señalar que se podría hacer una limpieza de estos datos para unificarlos porque actualmente no se puede hacer nada con ellos, pero debido a que faltan la mitad de los registros, podemos eliminar esta variable del dataset, de forma que lo simplificariamos un poco más.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Floor))
```

## Análisis variables numéricas

### Referencia

Este columna es un valor unico que se utiliza en la web de Tecnocasa como idnetificador. No nos sirve para el análisis y se puede eliminar. 

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Reference))
```

### Fecha de Publicación

Este columna es un valor que registra cuando se publicó en la web. Realmente está en este dataset para luego ampliar el análisis y considerar si sigue publicado o no, por lo que se va a eliminar del dataset

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Publish_date))
```

### Energy_Consumption

```{r}
summary(properties_spain_sell$Energy_Consumption)
```

Si que me hubiera gustado meter esta variable en el conjunto de entrenamiento, pero faltan la mitad de los datos por lo que no merece la pena, además, posiblemente el consumo energético dependa más de cada habitante de la propiedad que las métricas que de la inmobiliaria, por lo que se va a eliminar del dataset.

```{r}
properties_spain_sell <- subset(properties_spain_sell, select = -c(Energy_Consumption))
```

### Superficie Construida

```{r}
variable <- properties_spain_sell$Surface

hist(variable, 
     breaks = 50,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Superficie Construida", 
     xlab = "Superficie Construida", 
     ylab = "Densidad")

lines(density(variable, na.rm = TRUE), 
      col = "red",          
      lwd = 2)             

```

Como vemos tenemos una gráfica con una gran cola a la derecha, por lo que se van a ver los outliers y se va a proceder a eliminarlos del dataset.

```{r}
boxplot(properties_spain_sell$Surface, 
        main = "Boxplot de la superficie construida", 
        ylab = "Superficie", 
        col = "lightblue", 
        notch = TRUE)

```

En vista de estos datos, vamos a eliminar los registros que tienen una superficie inferior a 500 m2 y volveremos a analizar todo a ver como queda de cara que luego podamos analizar estudiar los outliers de forma multivariante.

```{r}
#properties_spain_sell <- properties_spain_sell[properties_spain_sell$Surface <= 500,]
```

```{r}
variable <- properties_spain_sell$Surface

hist(variable, 
     breaks = 50,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Superficie Construida", 
     xlab = "Superficie Construida", 
     ylab = "Densidad")

lines(density(variable, na.rm = TRUE), 
      col = "red",          
      lwd = 2)             

```

```{r}
boxplot(properties_spain_sell$Surface, 
        main = "Boxplot de la superficie construida", 
        ylab = "Superficie", 
        col = "lightblue", 
        notch = TRUE)

```

Sigue habiendo outliers, pero hemos eliminado gran parte de los mismos, así que por ahora, se va a dejar.



### Año de construcción

```{r}
summary(properties_spain_sell$Year_Construction)
```


```{r}
variable <- properties_spain_sell$Year_Construction 


hist(variable, 
     breaks = 100,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución de Año de Construcción", 
     xlab = "Año de Construcción", 
     ylab = "Densidad")

# Añadir la curva suavizada (density)
lines(density(variable, na.rm = TRUE), 
      col = "red",          # Color de la curva
      lwd = 2)              # Ancho de la línea

```


Respecto a esta variable, vemos que mayoritariamente los pisos se construyeron entre 1960 y los 1980, condicionando con el boom económico español. También se ve reflejada la crisis del 2008 donde se disminuyo dramáticamente la construcción de nuevas propiedades y como a día de hoy todavía no ha vuelto a repuntar. 

```{r}
boxplot(properties_spain_sell$Year_Construction, 
        main = "Boxplot del Año de Construcción", 
        ylab = "Año de Construcción", 
        col = "lightblue", 
        notch = TRUE)
```

Nos encontramos con varios outliers en este dataset respecto al año de construcción, al ser pocos podemos eliminarnos del modelo en el analisis de outliers multivariente posterior y vemos que nos faltan 3 años, los cuales podremos imputarlos despues del analisis de outliers.

### Baños


```{r}
summary(properties_spain_sell$Bathrooms)
```
Tenemos 60 valores nulos en esta variable de 370, lo que equivale a un 16% del dataset, por lo que se puede imputar los valores con el fin de completar esta variable. Al estar con una variable tipo factor, podemos usar o la moda (donde imputariamos todos los registros a que tengan 1 baño) o un método predictivo como el k-NN.

Posiblemente, la imputación con la moda es probablemente el enfoque más sencillo y el más fácil de usar en este caso, puesto que la variable Bathrooms tiene pocos factores distintos.

```{r}
mode_bathrooms <- as.numeric(names(sort(table(properties_spain_sell$Bathrooms), decreasing = TRUE)[1]))

properties_spain_sell$Bathrooms[is.na(properties_spain_sell$Bathrooms)] <- mode_bathrooms
```

```{r}
summary(properties_spain_sell$Bathrooms)
```
```{r}
ggplot(properties_spain_sell, aes(x = Bathrooms, fill = factor(Bathrooms))) +
  geom_bar() +
  xlab("Baños") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de baños por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Baños")
```


### Habitaciones


```{r}
summary(properties_spain_sell$Bedrooms)
```
Tenemos 53 valores nulos en esta variable de 370, lo que equivale a un 13% del dataset. 

```{r}
ggplot(properties_spain_sell, aes(x = Bedrooms, fill = factor(Bedrooms))) +
  geom_bar() +
  xlab("Habitaciones") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de Habitaciones por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Habitaciones")
```

Al igual que en el caso anterior, se va a imputar el valor "Na" por la moda del número de habitaciones, el cual es 3, en este caso. 

```{r}
moda <- as.numeric(names(sort(table(properties_spain_sell$Bedrooms), decreasing = TRUE)[1]))

properties_spain_sell$Bedrooms[is.na(properties_spain_sell$Bedrooms)] <- moda

properties_spain_sell$Bedrooms <- droplevels(properties_spain_sell$Bedrooms)
```


```{r}
summary(properties_spain_sell$Bedrooms)
```


```{r}
ggplot(properties_spain_sell, aes(x = Bedrooms, fill = factor(Bedrooms))) +
  geom_bar() +
  xlab("Habitaciones") + 
  ylab("Frecuencia") +
  ggtitle("Distribución de Habitaciones por propiedad")+
  theme_minimal() +
  theme(panel.grid = element_blank(),
      axis.line = element_line(colour = "black")) + 
  scale_fill_brewer(palette = "Set3") +   
  labs(fill = "Número de Habitaciones")
```



### Precio de Venta

El precio de venta es la variable objetivo.

```{r}
summary(properties_spain_sell$Sale_Price)
```
Vemos, en este punto que el máximo está muy lejos del tercer cuartil, por lo que se va a tener que eliminar, porque debe ser un punto atípico. Veamos antes como son las distribución de esta varaible.


```{r}
variable <- properties_spain_sell$Sale_Price

hist(variable, 
     breaks = 100,
     probability = TRUE,
     col = "lightblue",
     main = "Distribución del Precio de Venta", 
     xlab = "Precio de venta", 
     ylab = "Densidad")

# Añadir la curva suavizada (density)
lines(density(variable, na.rm = TRUE), 
      col = "red",          # Color de la curva
      lwd = 2)              # Ancho de la línea

```

Lo que se puede ver en esta variables es que la mayoría de las viviendas tienen un precio de hasta 40.000 €, lo cual puede ser indicativo del estado, año de construcción u otras características...

```{r}

boxplot(properties_spain_sell$Sale_Price, 
        main = "Boxplot del Precio de Venta", 
        ylab = "Precio de Venta", 
        col = "lightblue", 
        notch = TRUE)

```

Vemos que existen outliers de manera única, muy alejados del resto, por lo que se van a eliminar.

```{r}
Q1 <- quantile(properties_spain_sell$Sale_Price, 0.25)
Q3 <- quantile(properties_spain_sell$Sale_Price, 0.75)
IQR_value <- IQR(properties_spain_sell$Sale_Price)

# Definir los límites inferior y superior
lower_limit <- Q1 - 1.5 * IQR_value
upper_limit <- Q3 + 1.5 * IQR_value

# Eliminar los valores atípicos (outliers)
properties_spain_sell <- properties_spain_sell[properties_spain_sell$Sale_Price >= lower_limit & properties_spain_sell$Sale_Price <= upper_limit,]
```



```{r}

boxplot(properties_spain_sell$Sale_Price, 
        main = "Boxplot del Precio de Venta", 
        ylab = "Precio de Venta", 
        col = "lightblue", 
        notch = TRUE)

```


## Analisis del dataset limpio


```{r}
skim(properties_spain_sell)
```
Vemos que no hay inmuebles con 4 baños, por lo que vamos a eliminarlo del factor.

```{r}
properties_spain_sell$Bathrooms <- droplevels(properties_spain_sell$Bathrooms)
```


Los valores que nos faltan del año de construcción se podrían imputar o eliminar. En este caso, por simplifidad, se van a eliminar.

```{r}
properties_spain_sell <- properties_spain_sell[!is.na(properties_spain_sell$Year_Construction),]
```


### Detección de Outliers: PCA y distancia de Mahalanobis

Para analizar los outliers de este dataset de forma conjunta se va a utilizar el análisis de componentes principales (PCA, por sus siglas en inglés). Vamos a convertir cada variable factor en una columna binaria para evitar suposiciones de orden.


```{r}
properties_spain_encoded <- dummy_cols(properties_spain_sell, 
                                       select_columns = c("Bathrooms", "Bedrooms", "Property"),
                                       remove_selected_columns = TRUE)

# Seleccionar las variables numéricas
numeric_vars <- properties_spain_encoded[, c("Year_Construction", "Sale_Price", "Surface")]

# Normalizar las variables
numeric_vars_scaled <- scale(numeric_vars)

# Reemplazar en el dataset
properties_spain_encoded[, c("Year_Construction", "Sale_Price", "Surface")] <- numeric_vars_scaled

```


```{r}
# Aplicar PCA
pca_result <- prcomp(properties_spain_encoded, scale. = TRUE)

# Resumen de la varianza explicada
summary(pca_result)
``` 


```{r}
# Obtener la desviación estándar de cada componente
sdev <- pca_result$sdev

# Calcular la varianza explicada (al cuadrar la desviación estándar)
var_explained <- sdev^2

# Calcular la proporción de varianza explicada
prop_var_explained <- var_explained / sum(var_explained)

# Calcular la varianza acumulada
cumulative_var_explained <- cumsum(prop_var_explained)

# Graficar la varianza acumulada
plot(cumulative_var_explained, type="b", main="Cumulative Proportion of Variance Explained",
     xlab="Principal Components", ylab="Cumulative Proportion of Variance", col="blue", pch=19)

```


```{r}
# Obtener la desviación estándar de cada componente
sdev <- pca_result$sdev

# Calcular la varianza explicada (al cuadrar la desviación estándar)
var_explained <- sdev^2

# Calcular la proporción de varianza explicada
prop_var_explained <- var_explained / sum(var_explained)

plot(prop_var_explained, type="b", pch=19, col="blue", xlab="Número de componentes principales", 
     ylab="Proporción de varianza explicada", main="Gráfico de la proporción de varianza explicada")

```

En vista de este análisis 11 PCA explicarían el 90% de la varianza de los datos y podríamos usarlo para eliminar los outliers del modelo.


```{r}
pca_data <- data.frame(pca_result$x[, 1:11])
```


```{r}
# Calcular la matriz de covarianza y la distancia de Mahalanobis
cov_matrix <- cov(pca_data) 

# Calcular la media de las puntuaciones de PCA
mean_data <- colMeans(pca_data)

# Calcular la distancia de Mahalanobis para cada observación
mahal_dist <- mahalanobis(pca_data, center = mean_data, cov = cov_matrix)

# Definir el umbral usando la distribución chi-cuadrada
threshold <- qchisq(0.95, df = 12) 

# Identificar los outliers
outliers <- mahal_dist > threshold

# Ver los índices de los outliers
which(outliers)
```

Dibujamos algunos de los PCA para ver como se reparten los outliers, previo borrado del dataset.

```{r}
pca_data$outlier <- outliers
```

```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 1 (PC1)",
    y = "Componente Principal 2 (PC2)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```
```{r}
ggplot(pca_data, aes(x = PC1, y = PC3, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 1 (PC1)",
    y = "Componente Principal 3 (PC3)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```
```{r}
ggplot(pca_data, aes(x = PC2, y = PC3, color = outlier)) +
  geom_point(size = 2) +
  labs(
    title = "Visualización de Outliers en los Componentes Principales (PCA)",
    x = "Componente Principal 2 (PC2)",
    y = "Componente Principal 3 (PC3)",
    color = "Outlier"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"))
```

Si que vemos en las gráficas que hay una gran dispersión en los datos atípicos que pueden distorsionarnos los resultados, por lo que, finalmente eliminamos los outliers del dataset para poder trabajar con en los posteriores análisis.



```{r}
cleaned_properties <- properties_spain_sell[!outliers, ]
```

Vemos, tras el análisis, que tenemos varios puntos que son outliers en nuestro dataset y que, por tanto, debemos eliminar del modelo antes de proceder a realizar el análisis de los datos. Además lo borramos del dataset que ya tenemos preparado con todas las variables numéricas, lo cual nos va a servir para aplicar los modelos de análisis posteriores.

```{r}
skim(cleaned_properties)
```

Vemos que no hay inmuebles con 1 habitación, por lo que vamos a eliminarlo del factor.

```{r}
cleaned_properties$Bedrooms <- droplevels(cleaned_properties$Bedrooms)
```


```{r}
write.csv(cleaned_properties, file = "cleaned_properties_Spain.csv", row.names = FALSE)
```


# Análisis de los datos

El objetivo de este análisis es predecir una variable numérica continua ("Sale_Price"), por lo que los métodos adecuados podrían ser:

- Métodos Supervisados 
  1. Regresión Lineal Múltiple
  2. Árboles de Decisión
  3. Random Forest
  4. Gradient Boosting
  5. Red Neuronal

## Separación de la variable objetivo y llas varaibles predictoras

```{r}
obs_X <- subset(cleaned_properties, select = c(-Sale_Price))
obj_Y <- subset(cleaned_properties, select = c(Sale_Price))
```

## Reescalado de las variables predictoras

```{r}
obs_X_encoded <- dummy_cols(obs_X, 
                            select_columns = c("Bathrooms", "Bedrooms", "Property"),
                            remove_first_dummy = TRUE,  # Opcional: evita colinealidad
                            remove_selected_columns = TRUE)

# Seleccionar las variables numéricas
numeric_vars <- obs_X_encoded[, c("Year_Construction", "Surface")]

# Normalizar las variables
numeric_vars_scaled <- scale(numeric_vars)

# Reemplazar en el dataset
obs_X_encoded[, c("Year_Construction", "Surface")] <- numeric_vars_scaled

```


## Colinealidad de las variables
  

Lo primero que vamos a analizar es si con las variables que tenemos, presentamos colinealidad con el fin de eliminarla antes de realizar los métodos supervisados y no supervisados.


```{r}
cor(obs_X_encoded)
```
```{r}
data_encoded <- obs_X_encoded
data_encoded$Sale_Price <- obj_Y$Sale_Price
vif(lm(Sale_Price ~ ., data = data_encoded))
```
Si analizamos el VIF y la matriz de colinealidad, vemos que no se presenta colinealidad entre los datos elegidos, por lo que podemos seguir adelante sin eliminar varaibles del modelo.



## Métodos Supervisados

Respecto a los métodos supervisados, se va a aplicar en primera instancia una Regresión Lineal, por tantear el comportamiento de los datos para, después aplicar uno de los otros métodos.

### Regresión Lineal

```{r}
model <- lm(Sale_Price ~ ., data = data_encoded)
summary(model)
```
Aunque no tenemos mucha información de este modelo y, realmente no nos sirve, si que podemos intuir que el numero de baños, si es casa o piso y la superficie construida influyen significativamente en el precio de la vivienda.


```{r}
predicted_prices <- predict(model, newdata = data_encoded)
# Calcular el RMSE
actual_prices <- cleaned_properties$Sale_Price
rmse_lm <- sqrt(mean((actual_prices - predicted_prices)^2))
print(paste("RMSE para la regresión lineal:", rmse_lm))
```
```{r}
ggplot(data = data_encoded, aes(x = actual_prices, y = predicted_prices)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  xlab("Precios reales") +
  ylab("Precios predichos") +
  ggtitle("Comparación de precios reales vs. predichos") +
  theme_minimal()
```


### K-NN

```{r}
set.seed(42)

train_index <- sample(1:nrow(obs_X_encoded), 0.8 * nrow(obs_X_encoded))
train_data <- obs_X_encoded[train_index, ]
test_data <- obs_X_encoded[-train_index, ]

# Variables objetivo para entrenamiento y prueba
train_target <- obj_Y$Sale_Price[train_index]
test_target <- obj_Y$Sale_Price[-train_index]
```


```{r}
# Probar diferentes valores de k
rmse_values <- c()
for (k in 1:50) {
  knn_model <- knn.reg(train = train_data, test = test_data, y = train_target, k = k)
  predictions <- knn_model$pred
  rmse <- sqrt(mean((test_target - predictions)^2))
  rmse_values <- c(rmse_values, rmse)
}

# Graficar RMSE vs k
plot(1:50, rmse_values, type = "b", xlab = "Número de vecinos (k)", ylab = "RMSE")


```
```{r}
k <- 30

knn_model <- knn.reg(train = train_data, test = test_data, y = train_target, k = k)

# Predicciones
predictions <- knn_model$pred

rmse <- sqrt(mean((test_target - predictions)^2))
print(paste("RMSE:", rmse))

# Calcular R²
sst <- sum((test_target - mean(test_target))^2)
sse <- sum((test_target - predictions)^2)
r_squared <- 1 - (sse / sst)
print(paste("R²:", round(r_squared,4)))
```


## Métodos no supervisados

Para los métodos no supervisados 

### K-Means

```{r}
# Calcular la suma de las distancias intra-clúster (inertia) para diferentes k
wss <- sapply(1:15, function(k) {
  kmeans(obs_X_encoded, centers = k, nstart = 25)$tot.withinss
})

# Graficar el método del codo
plot(1:15, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de Clústeres (k)", ylab = "Suma de Cuadrados Intra-Clúster (WSS)")
```

```{r}
predictors <- obs_X_encoded

# Probar diferentes valores de k y calcular la silueta promedio
sil_width <- sapply(2:10, function(k) {
  pam(predictors, k = k)$silinfo$avg.width
})

# Graficar el coeficiente de silueta
plot(2:10, sil_width, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de Clústeres (k)", ylab = "Coeficiente de Silueta")
```
```{r}
k = 8

kmeans_model <- kmeans(predictors, centers = k, nstart = 25)

# Agregar los clústeres al dataset original
predictors$Cluster <- kmeans_model$cluster
```

```{r}
table(kmeans_model$cluster)
```
```{r}
aggregate(predictors[, -ncol(predictors)], by = list(Cluster = predictors$Cluster), FUN = mean)
```

```{r}
# Visualizar los clústeres en 2D
ggplot(predictors, aes(x = Surface, y = Year_Construction, color = factor(Cluster))) +
  geom_point() +
  labs(color = "Cluster") +
  theme_minimal()
```
```{r}
aggregate(obj_Y$Sale_Price, by = list(Cluster = predictors$Cluster), FUN = mean)
```


Razón para excluir Sale_Price

K-Means crea clústeres basados en la distancia:

Si incluyes Sale_Price, el algoritmo tenderá a agrupar propiedades según esta variable en lugar de considerar un balance de todas las características. Esto podría resultar en clústeres sesgados o poco interpretables.

La variable Sale_Price ya es una función de otras variables:

En tu análisis supervisado, Sale_Price es el resultado de una combinación de las otras variables. Al incluirla en K-Means, estás introduciendo redundancia y potencialmente distorsionando los clústeres.

### DBSCAN

```{r}
predictors <- obs_X_encoded
```


```{r}
dbscan_model <- dbscan(predictors, eps = 1, minPts = 5)

# Ver los resultados
print(dbscan_model)

# Asignar los clústeres al dataset original
predictors$Cluster <- dbscan_model$cluster

```


```{r}
# Valores de `eps` y `minPts` a probar
eps_values <- seq(0.02, 2, by = 0.02)  # Cambia según el rango esperado de `eps`
minPts_values <- c(5, 10, 15, 20)       # Cambia según el dataset

# Dataframe para almacenar resultados
results <- data.frame(
  eps = numeric(0),
  minPts = numeric(0),
  clusters = integer(0),
  noise_points = integer(0)
)

# Iterar sobre los valores
for (eps in eps_values) {
  for (minPts in minPts_values) {
    predictors <- obs_X_encoded
    # Aplicar DBSCAN
    dbscan_model <- dbscan(predictors, eps = eps, minPts = minPts)
    
    # Contar número de clústeres (excluyendo el ruido, que es cluster 0)
    num_clusters <- length(unique(dbscan_model$cluster[dbscan_model$cluster > 0]))
    
    # Contar puntos de ruido
    noise_points <- sum(dbscan_model$cluster == 0)
    
    # Guardar resultados
    results <- rbind(
      results,
      data.frame(
        eps = eps,
        minPts = minPts,
        clusters = num_clusters,
        noise_points = noise_points
      )
    )
  }
}
```


```{r}
ggplot(results, aes(x = eps, y = clusters, color = factor(minPts))) +
  geom_line() +
  geom_point() +
  labs(
    title = "Número de clústeres vs. eps y minPts",
    x = "eps",
    y = "Número de clústeres",
    color = "minPts"
  ) +
  theme_minimal()
```

```{r}
ggplot(results, aes(x = eps, y = noise_points, color = factor(minPts))) +
  geom_line() +
  geom_point() +
  labs(
    title = "Número de puntos de ruido vs. eps y minPts",
    x = "eps",
    y = "Número de puntos de ruido",
    color = "minPts"
  ) +
  theme_minimal()

```

```{r}
results_filtered <- results[results$clusters > 2,]
# Ver los resultados
min <- min(results_filtered$noise_points)
optimal_config <- results_filtered[results_filtered$noise_points == min,]
print(optimal_config)
```


```{r}
optimal_eps <- optimal_config$eps[1]
optimal_minPts <- optimal_config$minPts[1]

dbscan_model <- dbscan(predictors, eps = optimal_eps, minPts = optimal_minPts)

```

```{r}
predictors$Cluster <- kmeans_model$cluster

ggplot(predictors, aes(x = Surface, y = Year_Construction, color = factor(Cluster))) +
  geom_point() +
  labs(color = "Cluster") +
  theme_minimal()
```

```{r}
aggregate(obj_Y$Sale_Price, by = list(Cluster = predictors$Cluster), FUN = mean)
```
```



## Prueba por contraste de hipótesis



